# The replication crisis

The way we teach children about science goes something like this: science is a noble quest for the truth. To achieve this, scientists use something called the _scientific method_, which involves observations, hypotheses and experiments. Then scientists use a kind of mathematics called _statistics_ which guarantees that only true results get published.

If you believed this, then you would have been surprised in 2005 when  J. P. A. Ioannidis published a paper called [Why Most Published Research Findings
Are False](https://journals.plos.org/plosmedicine/article/file?id=10.1371/journal.pmed.0020124&type=printable). This was one of the papers that led to the term _replication crisis_, a realisation among researchers that formulas such as `p < 0.05` do not always mean what you want them to.

Psychology was the first field to come under scrutiny for results that do not replicate, but the problem has since been found in many other fields from medical research to social sciences. A more detailed explanation from an online textbook for psychology undergradutes discusses [the replication crisis](https://nobaproject.com/modules/the-replication-crisis-in-psychology) and suggests that, depending on the journal, only 23%-53% of studies will replicate along with possible reasons for this.

The learning outcome for this week is to understand some of the limitations of the scientific/statistical method, both for when you do your own experiements (e.g. in your final project) and for when you read scientific papers (e.g. for the background section of your final project).

## Readings

There are no new lectures for this week as we are not introducing any new maths content. However, we are studying how to interpret scientific/statistical results, and p-values in particular, so please read the following:

**Online textbook article**: [the replication crisis in psychology](https://nobaproject.com/modules/the-replication-crisis-in-psychology)

This is aimed at undergraduates, and has very little mathematics in it (if any), but it does give an introduction in plain English to the problem and some possible solutions.

After reading this, you should be able to answer the following questions:

  - What is replication (of a scientific paper) and why is it important?
  - What is the difference between exact and conceptual replications, and what different things do the two tell us?
  - What is priming (in psychology) and what are some examples of studies that have failed to replicate, which was one of the causes of the replication crisis?
  - What are some reasons why an experiment might not replicate, even if the scientists are being honest?
  - What are some possible solutions to the replication problem?

**Paper**: [Why Most Published Research Findings
Are False](https://journals.plos.org/plosmedicine/article/file?id=10.1371/journal.pmed.0020124&type=printable) (PDF, 6 pages).

This is the paper that raised awareness of the replication problem in medicine, and one of few papers to get famous enough that its title has become its own wikipedia page.

This paper does contain some mathematics, but it is basic probability theory that you should be able to follow.

After reading, you should be able to answer the following questions:

  - What is the definition and meaning of the quantity denoted beta in a hypothesis test?
  - What, in terms of simple probability theory, is the PPV of a study?
  - What influence does the typical effect size in a field of study have on the likelihood that published results are true?
  - What effect on the reliability of research does it have when a field becomes "hot", and why?
  - What does Ioannidis mean by "What matters is the totality of the evidence." on pages 5-6?

The point of this paper is _not_ that science is generally useless, or at least no better than any other method - on matters such as whether Covid-19, climate change, AIDS etc. are real and what their causes are, science does have very firm answers and I do not think Ioannidis would dispute this. But the paper is a warning against someone claiming something is an absolute truth and not up for debate, just because there are one or two published studies somewhere that support the view.

**Paper**: [Statistical tests, P values, confidence intervals, and power: a guide
to misinterpretations](https://link.springer.com/content/pdf/10.1007/s10654-016-0149-3.pdf) (PDF, 14 pages)

This 2016 paper by Greenland et al. in the European Journal of Epidemiology lists a number of misinterpretations that scientists have given of the meaning of the term _p-value_, as well as a correct explanation of what the p-value actually means. It is worth reading the introduction and conclusions in detail, but you can skim-read the list of false interpretations of p-values in the middle if you are short on time.

After reading this paper, you should be able to correctly define and explain what a p-value is.

## Why does this matter for me?

  - In later years, you will be reading research literature as part of your units and maybe also as part of your final project. Some of the papers you read will use statistics, and you will need to be able to read them critically and understand what a statistical claim does, or does not mean. Published and peer reviewed is not the same thing as definitely true!
  - If you take units in areas that use statistics directly - particularly Machine Learning and Data Science, but also Human-Computer Interaction - then you will need to understand the limits as well as the features of different methods. (Machine Learning actually comes off quite well, both due to large data sets and replication being eaiser compared to e.g. psychology. The coursework in our 4th year unit Applied Deep Learning in 2020-21 was in fact to replicate the work in a particular paper.)
  - If you run your own research and report statistics (e.g. in the evaluation section of your thesis), then you have an ethical responsibility to apply a certain amount of critical thinking to your own results too, as well as using best practice in your research area (such as publishing your code and datasets for other researchers to study and replicate, and not misinterpreting your own p-values). Your thesis markers are likely to be looking for this too.


